{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3lab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LM4w-gHqWyf",
        "outputId": "b1e60ac6-a895-4526-b9b8-16c37c33b2eb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylqpQGqQqVUW",
        "outputId": "f1f36d85-b97d-4c92-fd72-75493c39d706"
      },
      "source": [
        "%cd /content/drive/MyDrive/"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ohu9yXsrxTq"
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFR13MajmWWJ"
      },
      "source": [
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y2RaxRTnWCI"
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-9.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WSfWN7EnW2r",
        "outputId": "ff40045a-c601-4654-daba-1551301da3a8"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Wed_Apr_11_23:16:29_CDT_2018\n",
            "Cuda compilation tools, release 9.2, V9.2.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWeH_q6wnp3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70bcfa9-2582-4ad4-d827-5f6da4ad5ced"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-ojtc_pe7\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-ojtc_pe7\n",
            "Requirement already satisfied (use --upgrade to upgrade): NVCCPlugin==0.0.2 from git+git://github.com/andreinechaev/nvcc4jupyter.git in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=0743e11067589b0bed10c087ffd9106663e6c09f3b9682aeb7a1d0522aa649f2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cy2vr6uz/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjO8nl5qokx2",
        "outputId": "b3caa3f9-7370-492e-bceb-512ea395a85a"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLrIZAtGN6d-",
        "outputId": "45a1dcf0-b884-4117-e9f4-797fd2c070fd"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"chrono\"\n",
        "\n",
        "void matDist (float * a, float * b, int n, int BLOCK_SIZE, float * c) {\n",
        "    float sum;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            sum = 0.0f;\n",
        "            for (int k = 0; k < n; k++) {\n",
        "                sum += abs(a[i * n + j] - b[j * n + i]);\n",
        "            }\n",
        "            c[i * n + j] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void matDistGlobal ( float * a, float * b, int n, int BLOCK_SIZE, float * c ) {\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    float sum = 0.0f;\n",
        "    int ia = n * BLOCK_SIZE * by + n * ty;\n",
        "    int ib = BLOCK_SIZE * bx + tx;\n",
        "    int ic = ia + ib;\n",
        "    for (int k = 0; k < n; k++) {\n",
        "        sum += abs(a[ia + k] - b[ib + k*n]);\n",
        "    }\n",
        "    c[ic] = sum;\n",
        "}\n",
        "\n",
        "__global__ void matDistShared (float * a, float * b, int n, int BLOCK_SIZE, float * c) {\n",
        "    int bx = blockIdx.x, by = blockIdx.y;\n",
        "    int tx = threadIdx.x, ty = threadIdx.y;\n",
        "    int aBegin = n * 16 * by;\n",
        "    int aEnd = aBegin + n - 1;\n",
        "    int bBegin = 16 * bx;\n",
        "    int aStep = 16, bStep = 16 * n;\n",
        "    float sum = 0.0f;\n",
        "    __shared__ float as [16][17];\n",
        "    __shared__ float bs [16][17];\n",
        "    for ( int ia = aBegin, ib = bBegin; ia <= aEnd; ia += aStep, ib += bStep) {\n",
        "        as [ty][tx] = a [ia + n * ty + tx];\n",
        "        bs [ty][tx] = b [ib + n * ty + tx];\n",
        "        __syncthreads ();\n",
        "        for ( int k = 0; k < BLOCK_SIZE; k++) {\n",
        "            sum += abs(as[k][ty] - bs [tx][k]);\n",
        "        }\n",
        "        __syncthreads ();\n",
        "    }\n",
        "    c[aBegin + bBegin + n * ty + tx] = sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int BLOCK_SIZE = 16; int N = 1024;\n",
        "\n",
        "    int numBytes = N * N * sizeof(float);\n",
        "    float * h_A = (float*)malloc(numBytes);;\n",
        "    float * h_B = (float*)malloc(numBytes);;\n",
        "    float * h_C = (float*)malloc(numBytes);;\n",
        "    \n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            h_A[i *N + j] = rand();\n",
        "            h_B[j * N + i] = h_A[i * N + j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float * d_A; float * d_B; float * d_C;\n",
        "    \n",
        "    cudaMalloc((void**)&d_A, numBytes);\n",
        "    cudaMalloc((void**)&d_B, numBytes);\n",
        "    cudaMalloc((void**)&d_C, numBytes);\n",
        "    \n",
        "    dim3 threads(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 blocks(N / BLOCK_SIZE, N / BLOCK_SIZE);\n",
        "    \n",
        "    cudaMemcpy(d_A, h_A, numBytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, numBytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    matDist(h_A, h_B, N, BLOCK_SIZE, h_C);\n",
        "\n",
        "    auto stop = std::chrono::high_resolution_clock::now();\n",
        "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);\n",
        "\n",
        "    printf(\"Serial: %i\\n\\n\", duration.count());\n",
        "\n",
        "    auto startG = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    matDistGlobal<<<blocks,threads>>> (d_A, d_B, N, BLOCK_SIZE, d_C);\n",
        "\n",
        "    auto stopG = std::chrono::high_resolution_clock::now();\n",
        "    auto durationG = std::chrono::duration_cast<std::chrono::microseconds>(stopG - startG);\n",
        "\n",
        "    cudaMemcpy(h_C, d_C, numBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Global: %i\\n\\n\", durationG.count());\n",
        "\n",
        "    auto startS = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    matDistShared<<<blocks,threads>>> (d_A, d_B, N, BLOCK_SIZE, d_C);\n",
        "\n",
        "    auto stopS = std::chrono::high_resolution_clock::now();\n",
        "    auto durationS = std::chrono::duration_cast<std::chrono::microseconds>(stopS - startS);\n",
        "\n",
        "    cudaMemcpy(h_C, d_C, numBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Shared: %i\\n\\n\", durationS.count());\n",
        "\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial: 4883807\n",
            "\n",
            "Global: 156\n",
            "\n",
            "Shared: 26\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5B4Cc3cop5B",
        "outputId": "2b39ee23-6073-45a7-d2ac-6449e1c404e9"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"chrono\"\n",
        "\n",
        "void matDist (float * a, float * b, int n, int BLOCK_SIZE, float * c) {\n",
        "    float sum;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            sum = 0.0f;\n",
        "            for (int k = 0; k < n; k++) {\n",
        "                sum += abs(a[i * n + j] - b[j * n + i]);\n",
        "            }\n",
        "            c[i * n + j] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void matDistGlobal ( float * a, float * b, int n, int BLOCK_SIZE, float * c ) {\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    float sum = 0.0f;\n",
        "    int ia = n * BLOCK_SIZE * by + n * ty;\n",
        "    int ib = BLOCK_SIZE * bx + tx;\n",
        "    int ic = ia + ib;\n",
        "    for (int k = 0; k < n; k++) {\n",
        "        sum += abs(a[ia + k] - b[ib + k*n]);\n",
        "    }\n",
        "    c[ic] = sum;\n",
        "}\n",
        "\n",
        "__global__ void matDistShared (float * a, float * b, int n, int BLOCK_SIZE, float * c) {\n",
        "    int bx = blockIdx.x, by = blockIdx.y;\n",
        "    int tx = threadIdx.x, ty = threadIdx.y;\n",
        "    int aBegin = n * 16 * by;\n",
        "    int aEnd = aBegin + n - 1;\n",
        "    int bBegin = 16 * bx;\n",
        "    int aStep = 16, bStep = 16 * n;\n",
        "    float sum = 0.0f;\n",
        "    __shared__ float as [16][17];\n",
        "    __shared__ float bs [16][17];\n",
        "    for ( int ia = aBegin, ib = bBegin; ia <= aEnd; ia += aStep, ib += bStep) {\n",
        "        as [ty][tx] = a [ia + n * ty + tx];\n",
        "        bs [ty][tx] = b [ib + n * ty + tx];\n",
        "        __syncthreads ();\n",
        "        for ( int k = 0; k < BLOCK_SIZE; k++) {\n",
        "            sum += abs(as[k][ty] - bs [tx][k]);\n",
        "        }\n",
        "        __syncthreads ();\n",
        "    }\n",
        "    c[aBegin + bBegin + n * ty + tx] = sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int BLOCK_SIZE = 16; int N = 2048;\n",
        "\n",
        "    int numBytes = N * N * sizeof(float);\n",
        "    float * h_A = (float*)malloc(numBytes);;\n",
        "    float * h_B = (float*)malloc(numBytes);;\n",
        "    float * h_C = (float*)malloc(numBytes);;\n",
        "    \n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            h_A[i *N + j] = rand();\n",
        "            h_B[j * N + i] = h_A[i * N + j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float * d_A; float * d_B; float * d_C;\n",
        "    \n",
        "    cudaMalloc((void**)&d_A, numBytes);\n",
        "    cudaMalloc((void**)&d_B, numBytes);\n",
        "    cudaMalloc((void**)&d_C, numBytes);\n",
        "    \n",
        "    dim3 threads(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 blocks(N / BLOCK_SIZE, N / BLOCK_SIZE);\n",
        "    \n",
        "    cudaMemcpy(d_A, h_A, numBytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, numBytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    matDist(h_A, h_B, N, BLOCK_SIZE, h_C);\n",
        "\n",
        "    auto stop = std::chrono::high_resolution_clock::now();\n",
        "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);\n",
        "\n",
        "    printf(\"Serial: %i\\n\\n\", duration.count());\n",
        "\n",
        "    auto startG = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    matDistGlobal<<<blocks,threads>>> (d_A, d_B, N, BLOCK_SIZE, d_C);\n",
        "\n",
        "    auto stopG = std::chrono::high_resolution_clock::now();\n",
        "    auto durationG = std::chrono::duration_cast<std::chrono::microseconds>(stopG - startG);\n",
        "\n",
        "    cudaMemcpy(h_C, d_C, numBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Global: %i\\n\\n\", durationG.count());\n",
        "\n",
        "    auto startS = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    matDistShared<<<blocks,threads>>> (d_A, d_B, N, BLOCK_SIZE, d_C);\n",
        "\n",
        "    auto stopS = std::chrono::high_resolution_clock::now();\n",
        "    auto durationS = std::chrono::duration_cast<std::chrono::microseconds>(stopS - startS);\n",
        "\n",
        "    cudaMemcpy(h_C, d_C, numBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Shared: %i\\n\\n\", durationS.count());\n",
        "\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial: 38978654\n",
            "\n",
            "Global: 235\n",
            "\n",
            "Shared: 29\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK80eb0iowNx",
        "outputId": "a008b492-4a0c-4d3f-d99a-02262988abc6"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"chrono\"\n",
        "\n",
        "void matDist (float * a, float * b, int n, int BLOCK_SIZE, float * c) {\n",
        "    float sum;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            sum = 0.0f;\n",
        "            for (int k = 0; k < n; k++) {\n",
        "                sum += abs(a[i * n + j] - b[j * n + i]);\n",
        "            }\n",
        "            c[i * n + j] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void matDistGlobal ( float * a, float * b, int n, int BLOCK_SIZE, float * c ) {\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    float sum = 0.0f;\n",
        "    int ia = n * BLOCK_SIZE * by + n * ty;\n",
        "    int ib = BLOCK_SIZE * bx + tx;\n",
        "    int ic = ia + ib;\n",
        "    for (int k = 0; k < n; k++) {\n",
        "        sum += abs(a[ia + k] - b[ib + k*n]);\n",
        "    }\n",
        "    c[ic] = sum;\n",
        "}\n",
        "\n",
        "__global__ void matDistShared (float * a, float * b, int n, int BLOCK_SIZE, float * c) {\n",
        "    int bx = blockIdx.x, by = blockIdx.y;\n",
        "    int tx = threadIdx.x, ty = threadIdx.y;\n",
        "    int aBegin = n * 16 * by;\n",
        "    int aEnd = aBegin + n - 1;\n",
        "    int bBegin = 16 * bx;\n",
        "    int aStep = 16, bStep = 16 * n;\n",
        "    float sum = 0.0f;\n",
        "    __shared__ float as [16][17];\n",
        "    __shared__ float bs [16][17];\n",
        "    for ( int ia = aBegin, ib = bBegin; ia <= aEnd; ia += aStep, ib += bStep) {\n",
        "        as [ty][tx] = a [ia + n * ty + tx];\n",
        "        bs [ty][tx] = b [ib + n * ty + tx];\n",
        "        __syncthreads ();\n",
        "        for ( int k = 0; k < BLOCK_SIZE; k++) {\n",
        "            sum += abs(as[k][ty] - bs [tx][k]);\n",
        "        }\n",
        "        __syncthreads ();\n",
        "    }\n",
        "    c[aBegin + bBegin + n * ty + tx] = sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int BLOCK_SIZE = 16; int N = 4096;\n",
        "\n",
        "    int numBytes = N * N * sizeof(float);\n",
        "    float * h_A = (float*)malloc(numBytes);;\n",
        "    float * h_B = (float*)malloc(numBytes);;\n",
        "    float * h_C = (float*)malloc(numBytes);;\n",
        "    \n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            h_A[i *N + j] = rand();\n",
        "            h_B[j * N + i] = h_A[i * N + j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float * d_A; float * d_B; float * d_C;\n",
        "    \n",
        "    cudaMalloc((void**)&d_A, numBytes);\n",
        "    cudaMalloc((void**)&d_B, numBytes);\n",
        "    cudaMalloc((void**)&d_C, numBytes);\n",
        "    \n",
        "    dim3 threads(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 blocks(N / BLOCK_SIZE, N / BLOCK_SIZE);\n",
        "    \n",
        "    cudaMemcpy(d_A, h_A, numBytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, numBytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    matDist(h_A, h_B, N, BLOCK_SIZE, h_C);\n",
        "\n",
        "    auto stop = std::chrono::high_resolution_clock::now();\n",
        "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);\n",
        "\n",
        "    printf(\"Serial: %i\\n\\n\", duration.count());\n",
        "\n",
        "    auto startG = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    matDistGlobal<<<blocks,threads>>> (d_A, d_B, N, BLOCK_SIZE, d_C);\n",
        "\n",
        "    auto stopG = std::chrono::high_resolution_clock::now();\n",
        "    auto durationG = std::chrono::duration_cast<std::chrono::microseconds>(stopG - startG);\n",
        "\n",
        "    cudaMemcpy(h_C, d_C, numBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Global: %i\\n\\n\", durationG.count());\n",
        "\n",
        "    auto startS = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    matDistShared<<<blocks,threads>>> (d_A, d_B, N, BLOCK_SIZE, d_C);\n",
        "\n",
        "    auto stopS = std::chrono::high_resolution_clock::now();\n",
        "    auto durationS = std::chrono::duration_cast<std::chrono::microseconds>(stopS - startS);\n",
        "\n",
        "    cudaMemcpy(h_C, d_C, numBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Shared: %i\\n\\n\", durationS.count());\n",
        "\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial: 314069477\n",
            "\n",
            "Global: 933\n",
            "\n",
            "Shared: 23\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw7Wf9_QOBXT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}